### 强化学习学习手册



基本概念

机器学习分为：监督学习，无监督学习，半监督学习（也可以用hinton所说的强化学习）等。

**监督学习**（supervised learning）：使用已知正确答案的示例来训练网络。

在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）

有监督学习是从外部监督者提供的带标注训练集进行学习，所谓标注，即针对当前情境，系统应做出的正确动作。采用这种学习方式是为了让系统能够具备推断或泛化能力，能够响应不同的情境并做出正确的动作，哪怕这个情境没有在训练集合中出现过。这种学习方式不适合交互式学习，不可能获得在所有情况下既正确又有代表性的动作。



**无监督学习**：

在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。

无监督学习是一个典型的寻找未标注数据中隐含结构的过程。强化学习尽管也是无标注样本学习，但是其目的是最大化收益信号，而不是找出数据的隐含结构。



**半监督学习**：


在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。

 

**强化学习**：


在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）



学习者不会被告知应该采取什么动作，而是必须自己通过尝试去发现哪些动作会产生最丰厚的收益，同时，动作往往影响的不仅仅是即时收益，也会影响下一个情境，从而影响随后的收益。因此，**试错**和**延时收益**是强化学习两个最重要最显著的特征。



强化学习（Reinforcement Learning，又称为增强学习）这一名词来源于行为心理学，表示生物为了趋利避害而更加频繁实施对自己有利的策略。



强化学习的关键元素：

- 奖励（reward）
- 策略（policy）



**智能体**（agent）是强化学习系统中的决策者和学习者，它可以做出决策和接受奖励信号。

**环境**（environment）是强化学习系统中除智能体以外的所有事物，它是智能体交互的对象。



除了智能体和环境之外，强化学习系统有四个核心要素：**策略**、**收益信号**、**价值函数**和**环境模型**（可选）。

**策略**定义了学习智能体在特定时间的行为方式。简单地说，策略是环境状态到动作的映射，某些情况下，策略可以是一个简单的函数或者查询表，而另一些情况，策略可能涉及大量的运算。

**收益信号**定义了强化学习问题的目标。在每一步中，环境向强化学习智能体发送一个称为收益的标量数值。智能体的唯一目标是最大化长期总收益。

收益信号表明了在短时间内什么是好的，而**价值函数**则表示了从长远角度看什么是好的。一个状态的价值是一个智能体从当前状态开始，对将来累积的总收益的期望。



智能体观测环境，可以获得环境的观测（observation），记为O；

智能体根据观测做出决策，决定要对环境施加的动作（action），记为A；

环境受智能体动作的影响，改变自己的状态（state），记为S，并给出奖励（reward），记为R

